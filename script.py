import requests
from bs4 import BeautifulSoup
import os

BLOG_URL = "https://blog.goo.ne.jp/shinanren"

# ã‚«ã‚¹ã‚¿ãƒ  User-Agentï¼ˆé€šå¸¸ã®ãƒ–ãƒ©ã‚¦ã‚¶ã‚’è£…ã£ãŸã‚‚ã®ï¼‰
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
    "Referer": "https://www.google.com/"  # Googleæ¤œç´¢çµŒç”±ã‚’è£…ã†
}

def get_latest_article():
    """æœ€æ–°è¨˜äº‹ã®ã‚¿ã‚¤ãƒˆãƒ«ã¨ãƒªãƒ³ã‚¯ã‚’å–å¾—ã™ã‚‹"""
    print("ğŸ” ãƒ–ãƒ­ã‚°ã«ã‚¢ã‚¯ã‚»ã‚¹ä¸­...")
    
    # Session ã‚’ä½¿ç”¨ã—ã¦ Cookie ã‚’ä¿æŒ
    session = requests.Session()
    response = session.get(BLOG_URL, headers=HEADERS)
    
    print(f"âœ… HTTPã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰: {response.status_code}")
    
    if response.status_code != 200:
        raise ValueError(f"âŒ ãƒ–ãƒ­ã‚°ã«ã‚¢ã‚¯ã‚»ã‚¹ã§ãã¾ã›ã‚“ï¼ï¼ˆã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰: {response.status_code}ï¼‰")

    soup = BeautifulSoup(response.text, "html.parser")

    print("ğŸ” å–å¾—ã—ãŸHTMLã®ä¸€éƒ¨:")
    print(soup.prettify()[:1000])  # æœ€åˆã®1000æ–‡å­—ã ã‘å‡ºåŠ›ï¼ˆé•·ã™ãã‚‹ã¨GitHub Actionsã®ãƒ­ã‚°ã«å½±éŸ¿ï¼‰

    # è¨˜äº‹ãƒªã‚¹ãƒˆã® div ã‚’æ¢ã™
    article_list = soup.find("div", class_="blog_index_main")
    if article_list is None:
        raise ValueError("âŒ ãƒ–ãƒ­ã‚°ã®HTMLæ§‹é€ ãŒå¤‰æ›´ã•ã‚ŒãŸå¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ï¼")

    # è¨˜äº‹ã®ãƒªãƒ³ã‚¯ã‚’æ¢ã™
    link_tag = article_list.find("div", class_="blog_index_title").find("a")
    if link_tag is None:
        raise ValueError("âŒ è¨˜äº‹ã®ãƒªãƒ³ã‚¯ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼")

    latest_link = link_tag["href"]
    latest_title = link_tag.text.strip()

    print(f"âœ… æœ€æ–°è¨˜äº‹: {latest_title} ({latest_link})")
    return latest_link, latest_title
