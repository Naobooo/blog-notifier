import requests
from bs4 import BeautifulSoup
import os

BLOG_URL = "https://blog.goo.ne.jp/shinanren"

# User-Agent ã‚’å¤‰æ›´ã—ã€Referer ã‚’è¨­å®š
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
    "Referer": "https://www.google.com/"
}

def get_latest_article():
    """æœ€æ–°è¨˜äº‹ã®ã‚¿ã‚¤ãƒˆãƒ«ã¨ãƒªãƒ³ã‚¯ã‚’å–å¾—ã™ã‚‹"""
    print("ğŸ” ãƒ–ãƒ­ã‚°ã«ã‚¢ã‚¯ã‚»ã‚¹ä¸­...")
    
    # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ä½¿ç”¨ã—ã¦ Cookie ã‚’ç¶­æŒ
    session = requests.Session()
    response = session.get(BLOG_URL, headers=HEADERS)
    
    print(f"âœ… HTTPã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰: {response.status_code}")
    
    if response.status_code != 200:
        print(f"âŒ ãƒ–ãƒ­ã‚°ã«ã‚¢ã‚¯ã‚»ã‚¹ã§ãã¾ã›ã‚“ï¼ï¼ˆã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰: {response.status_code}ï¼‰")
        return None, None

    soup = BeautifulSoup(response.text, "html.parser")

    print("ğŸ” å–å¾—ã—ãŸHTMLã®ä¸€éƒ¨:")
    print(soup.prettify()[:1000])  # æœ€åˆã®1000æ–‡å­—ã ã‘å‡ºåŠ›

    # è¨˜äº‹ãƒªã‚¹ãƒˆã® div ã‚’æ¢ã™
    article_list = soup.find("div", class_="blog_index_main")
    if article_list is None:
        print("âŒ ãƒ–ãƒ­ã‚°ã®HTMLæ§‹é€ ãŒå¤‰æ›´ã•ã‚ŒãŸå¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ï¼")
        return None, None

    # è¨˜äº‹ã®ãƒªãƒ³ã‚¯ã‚’æ¢ã™
    link_tag = article_list.find("div", class_="blog_index_title").find("a")
    if link_tag is None:
        print("âŒ è¨˜äº‹ã®ãƒªãƒ³ã‚¯ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼")
        return None, None

    latest_link = link_tag["href"]
    latest_title = link_tag.text.strip()

    print(f"âœ… æœ€æ–°è¨˜äº‹: {latest_title} ({latest_link})")
    return latest_link, latest_title

def main():
    print("ğŸš€ `script.py` ãŒå®Ÿè¡Œã•ã‚Œã¾ã—ãŸï¼")
    latest_link, latest_title = get_latest_article()
    if latest_link is None:
        print("âš ï¸ æœ€æ–°è¨˜äº‹ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’çµ‚äº†ã—ã¾ã™ã€‚")
        return
    print("ğŸ†• æœ€æ–°è¨˜äº‹ã®æƒ…å ±ãŒå–å¾—ã•ã‚Œã¾ã—ãŸï¼")

# ã‚¹ã‚¯ãƒªãƒ—ãƒˆå®Ÿè¡Œ
if __name__ == "__main__":
    main()
